# 게이트웨이 미들웨어

 - 기존에 만들어진 게이트웨이 미들웨어를 활용하는 방법으로 미들웨어 애플리케이션에서 하이닉스향(유엔젤 작업)에 포함된 UDS를 활용함

# 게이트웨이 MQTT 미들웨어

- nodeJS가 기존 바이너리 미들웨어를 `child process` 로 실행
- UDS 서버 소켓을 만들고 기존미들웨어에서부터 데이터를 받음
- MQTT브로커로 데이터 송출 (RAW 바이너리 상태)
- 플랫폼에서 데이터를 파싱하여 저장
- 플랫폼 : 바이너리 파싱 (Frame Packet -> Payload -> Acc Data) 문제 없음, CRC OK
- 플랫폼 : 가속도 데이터 포멧은 1패킷당 16개의  패킷이 들어있고, 256개의 Index 로 쪼개서 받음, 패킷 단위로 파싱, 2의 보수 연산
  - 3바이트에 1바이트 추가하여 Float형태의 자료형으로 변환 adData = Buffer.concat([val.acc, Buffer.alloc(1)]).readUInt32LE(0);
  - 2의 보수 연산 adData = adData > 2 ** 23 ? adData - 2 ** 24 : adData;
- 플랫폼 : 기존 하이닉스향에서 발생했던 게인 문제로 아래와 같은 환산식 적용
  - 환산식 : 3.3 x (1/0.0380) x (1/0.0264) x (AD Val / 2^23)
  - 약식 : AD Val / 2550

# 문제점

## 2019.05.29 일자

- 새로 설치한 센서 및 게이트웨이에서 3000~-3000 수준의 g값이 발생
- 김준범차장, 김건우차장이 데이터 확인 결과 게인 1000배 정도가 있는것으로 확인 (차트에서는 KETI와 새로 설치된 센서의 값이 많이 차이남. 검토 필요)
  -> 환산식 적용하지 않고 원래의 센서티비티를 적용
           acc.push(adData * (3 / 0.0264 / 2 ** 23));
- 데이터가 깨지는 문제 : twos complement에서 오류가 발생하는 것으로 판단하여, 원본 소스를 모두 분석해봄
  - 게이트웨이에서 UDS를 통해 도달한 데이터를 파싱하여, 0-255 인덱스별로 저장
  - 0-255까지 모두 채워지는데 계측 시작 후 30분 이상 걸림 -> 중복데이터를 지속적으로 수신
  - 255개의 패킷이 모두 채워졌을 때 데이터의 과도한 시간차가 있음을 확인
  - UDS에서 쪼갠 패킷을 모두 보내지 않거나, 누락 되었거나, 순서가 뒤바뀜
  - 플랫폼에서는 패킷을 순서대로 정렬하지 못하고, 시간대별로 정확히 정렬할 수 없음
- 플랫폼 차트에서 중간에 비어있는 패킷에 과거 데이터가 채워져있거나 중복데이터로 반복됨
- twos complement 문제 미해결

## 2019.05.31 일자

- UDS 소켓확인을 위해 TCP로 변경 후 데이터 수집 : 동일 증상 발생
- UDS 소켓 on 데이터 이벤트에서 Redis Sorted Set으로 저장 후 MQTT로 송출하는 과정에서 패킷 손실이 있음을 확인 -> 수정: 버퍼를 수행하지 않고 이벤트 발생 후 데이터 바로 MQTT 브로커로 송출
- 데이터 중복 및 손실 문제 해결하였으나, 패킷은 순차적으로 보내지 않음을 확인 => nodeJS의 NET socket은 stream 개체로 readable, writable에 영향을 받음. 미들웨어에서 보내오는 데이터를 모두 MQTT로 pipe => MQTT QoS=2(데이터 전송 및 품질 보장)로 설정함에 따라 패킷은 순서대로 플랫폼에 도착하진 않음
- UDS소켓과 TCP소켓 모드 모두 구성

## 2019.06.01 ~ 06.03

- 설치 스크립트, cron, SSH 터널 스크립트 작성
- KETI 데이터 분석 (MATLAB) : 타임 스탬프 확인 결과 10ms 이내에 256개의 패킷은 모두 도착, 순서가 뒤바뀐 패킷은 sort가능

## 해결방안
- 정상적인 풀패킷 데이터를 MQTT브로커로 보내기 위한 방안 필요
- UDS에서 문제가 있어, 새롭게 게이트웨이 미들웨어를 개발할 필요가 있음
- 게이트웨이 미들웨어 개발 중
  - 게이트웨이 웹서버 구동 : 게이트웨이 설정, 센서 노드 등록, Passive 센서 데이터 폴링, Active Routine 설정 및 가동, 최신 데이터 차트
  - 게이트웨이 애플리케이션 : 애플리케이션 구동, 시스템 서비스 등록, MQTT 브로커 주소 및 토픽 설정, 추후 다른 브로커 및 메시징큐 옵션 추가

## 데이터 검증

- KETI 게이트웨이로 부터 수신한 데이터 : http://pdm.aproskorea.com/#/app/monitor/gateway/96:64:61:B7:EA:2E
- 새로 설치한 게이트웨이의 데이터
  - http://pdm.aproskorea.com/#/app/monitor/gateway/08:15:2F:59:A1:78
  - http://pdm.aproskorea.com/#/app/monitor/gateway/08:15:2F:59:A1:6B
  - http://pdm.aproskorea.com/#/app/monitor/gateway/08:15:2F:59:A1:82
- KETI의 경우 00124B000FAE9CAA번 센서가 4월4일 이후로 가동을 멈춤 -> 가동이 중단되기전 70g 정도 수준으ㄹOffset이 발생하고 있었음
- APROS_가속도계_연동_규격서_ADS127L01_v0.9.5.doc 문서를 참고하면 Auto Offset Calibration의 기능이 있고, 이를 게이트웨이가 요청을 하는지 알 수 없음
   > Auto Offset Calibration은 Offset 자동 설정을 위한 기능이며, 정현파가 아닌 계측 상태에서 Auto Offset Calibration을 시도할 시 부정확한 Offset이 설정될 수 있으므로, 정현파 Output(예: 가진기)을 통한 초기 원점 교정 시에만 사용한다. Auto Offset Calibration 수행 중에는 한 번의 계측이 수행된다. Auto Offset Calibration 수행 후 RAM내 계측 버퍼는 비워지며, Data Fetch를 위해서는 다시 한 번 Capture를 수행해야 한다.
- APROS_가속도계_연동_규격서_ADS127L01_v0.9.5.doc 문서를 참고하면 Set Offset 기능이 있음, 이 설정된 값을 Get Offset으로 받아서 확인할 필요가 있음